{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# 加载数据\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "# 增加维度， (60000, 28, 28) --> (60000, 28, 28, 1)\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 tf.data 创建batch和shuffle的数据集\n",
    "\n",
    "### tf.data 用于处理输入数据\n",
    "\n",
    "#### Dataset \n",
    "\n",
    "支持编写高性能的数据输入管道，可以按照下面的方式来使用:\n",
    "- 从自定义输入创建数据集\n",
    "- 应用数据转换进行数据预处理\n",
    "- 迭代式的处理数据（迭代式处理数据是目前流行的方式，避免全量加载到内存导致内存空间不足）\n",
    "\n",
    "直接基于数据创建数据集:\n",
    "```\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset:\n",
    "    print(element)\n",
    "```\n",
    "基于文件创建数据集\n",
    "```\n",
    "dataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])\n",
    "```\n",
    "基于TFRecord格式创建数据集\n",
    "```\n",
    "dataset = tf.data.TFRecordDataset([\"file1.tfrecords\", \"file2.tfrecords\"])\n",
    "```\n",
    "基于通配符文件创建数据集\n",
    "```\n",
    "dataset = tf.data.Dataset.list_files(\"/path/*.txt\")  # doctest: +SKIP\n",
    "```\n",
    "\n",
    "创建数据集后可以使用map进行转换操作:\n",
    "```\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "list(dataset.as_numpy_iterator())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建自定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(10)\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "    \n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择损失函数和优化器\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评价指标\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_func(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.021291617304086685, Accuracy 99.32666778564453, Test Loss 0.05509920045733452, Test Accuracy 98.3699951171875\n",
      "Epoch 2, Loss 0.01322136726230383, Accuracy 99.53333282470703, Test Loss 0.058831676840782166, Test Accuracy 98.29999542236328\n",
      "Epoch 3, Loss 0.009120644070208073, Accuracy 99.6883316040039, Test Loss 0.05732549726963043, Test Accuracy 98.43000030517578\n",
      "Epoch 4, Loss 0.00613583717495203, Accuracy 99.79000091552734, Test Loss 0.07237181812524796, Test Accuracy 98.11000061035156\n",
      "Epoch 5, Loss 0.006463431753218174, Accuracy 99.78666687011719, Test Loss 0.08328071236610413, Test Accuracy 98.19999694824219\n"
     ]
    }
   ],
   "source": [
    "E = 5\n",
    "for epoch in range(E):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "        \n",
    "    template = 'Epoch {}, Loss {}, Accuracy {}, Test Loss {}, Test Accuracy {}'\n",
    "    \n",
    "    print(template.format((epoch+1), \n",
    "                          train_loss.result(), \n",
    "                          train_accuracy.result()*100, \n",
    "                          test_loss.result(), \n",
    "                          test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            multiple                  320       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  2769024   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 2,770,634\n",
      "Trainable params: 2,770,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
