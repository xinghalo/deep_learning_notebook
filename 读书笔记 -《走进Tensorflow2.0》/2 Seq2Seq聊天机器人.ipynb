{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目参考:[https://github.com/zhaoyingjun/chatbot](https://github.com/zhaoyingjun/chatbot)\n",
    "\n",
    "\n",
    "语言模型：\n",
    "- 统计语言模型：根据贝叶斯公司计算每个单词出现的频率\n",
    "- n-gram语言模型：根据马尔科夫假设，每个词仅与前面n个词有关系，缩短概率依赖的词的数量。比如2-gram，表示每个词仅与前面一个词有关系；3-gram表示每个词仅与前面2个词有关系。\n",
    "- 神经网络语言模型：one-hot后输入神经网络，输出对应的向量作为表示\n",
    "\n",
    "[](https://www.bilibili.com/video/BV1o4411R7B1?from=search&seid=9886358575655357837)\n",
    "\n",
    "# RNN模型\n",
    "\n",
    "# LSTM模型\n",
    "\n",
    "# GRU模型\n",
    "\n",
    "# Seq2Seq模型\n",
    "\n",
    "# EncoderDecoder\n",
    "\n",
    "# Attention机制\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备，总共454130条\n",
    "lines = io.open('./2-train_data/seq.data', encoding='UTF-8').read().strip().split('\\n')\n",
    "# 如：继续\\t没有 继续 了\n",
    "preprocess_lines = [['start '+w+' end' for w in l.split('\\t')] for l in lines[:50000]]\n",
    "# 如：['start 继续 end', 'start 没有 继续 了 end']\n",
    "input_lang, target_lang = zip(*preprocess_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tf.keras.preprocessing.text.Tokenizer\n",
    "\n",
    "num_words: 保留词频最高的词的数量\n",
    "filters: 词典中需要过滤的词\n",
    "lower: 是否转换成小写\n",
    "split: 分词分隔符\n",
    "char_level: 如果为true，每个字符作为一个词\n",
    "oov_token: 用来代替没有出现在词典中的词(TODO 没弄明白这个是什么意思)\n",
    "\"\"\"\n",
    "# 分词，并转换成词典索引\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(input_lang + target_lang)\n",
    "x_tensor = tokenizer.texts_to_sequences(input_lang)\n",
    "y_tensor = tokenizer.texts_to_sequences(target_lang)\n",
    "# 序列填充成相同维度\n",
    "x_pad_tensor = tf.keras.preprocessing.sequence.pad_sequences(x_tensor, maxlen=20, padding='post')\n",
    "y_pad_tensor = tf.keras.preprocessing.sequence.pad_sequences(y_tensor, maxlen=20, padding='post')\n",
    "# 针对数据进行shuffle混排\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_pad_tensor, y_pad_tensor)).shuffle(len(x_pad_tensor))\n",
    "# 把数据切分成128大小\n",
    "dataset = datasets.batch(128, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 128, 20), (128, 128, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                       return_sequence=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, inital_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))\n",
    "    \n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "    \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        outputs = self.fc(output)\n",
    "        return outputs, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
